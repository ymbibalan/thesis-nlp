{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc700c2-d98b-4bc8-be18-1327dac69c2a",
   "metadata": {},
   "source": [
    "**Reference**:\n",
    "\n",
    "https://towardsdatascience.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface-94b2ed6f798f\n",
    "\n",
    "https://predictivehacks.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9040a6e2-3a79-4bd9-b7df-47819a718bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cabfdd39-ebb4-40d9-a637-37e841b213ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ü§ó Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74c3352e-ca99-42ac-81f8-8dce6bb19571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], [101, 11312, 18763, 10855, 11530, 112, 162, 39487, 10197, 119, 102, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    [\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "575ca416-3b33-4310-bb88-a8bff86ae91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff84807a-39bb-4ba8-8aff-96e4e336e407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens      :[101, 100, 2238, 4530, 2444, 100, 102]\n",
      "actual token:['[CLS]', '[UNK]', 'june', 'drop', 'live', '[UNK]', '[SEP]']\n",
      "revised actual tokens:['[CLS]', 'üö®', 'june', 'drop', 'live', 'üö®', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokens = tokenizer('üö® JUNE DROP LIVE üö®')['input_ids']\n",
    "actual_tokens = [tokenizer.decode(i) for i in tokenizer('üö® JUNE DROP LIVE üö®')['input_ids']]\n",
    "\n",
    "print(f'tokens      :{tokens}')\n",
    "print(f'actual token:{actual_tokens}')\n",
    "\n",
    "for i in ['üö®', 'üôÇ', 'üòç', '‚úåÔ∏è' , 'ü§© ']:\n",
    "    tokenizer.add_tokens(i)\n",
    "revised_actual_tokens = [tokenizer.decode(i) for i in tokenizer('üö® JUNE DROP LIVE üö®')['input_ids']]\n",
    "\n",
    "# Now, if you tokenize the sentence you will see that the emoji remains as emoji and not the [UNK] token.\n",
    "print(f'revised actual tokens:{revised_actual_tokens}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a68423f8-a8a4-41f3-ad95-06bbcd3b85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ = pd.read_csv('df_nlp_real.csv')\n",
    "# df_.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c7893e1-2566-4294-a134-827ce8f78ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>wip</th>\n",
       "      <th>activity_nlp</th>\n",
       "      <th>resource_nlp</th>\n",
       "      <th>case_nlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2 starts A3 on C1030 at 2010-02-22 12:05</td>\n",
       "      <td>48.0</td>\n",
       "      <td>A3</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R2 starts A1 on C1328 at 2010-02-22 13:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A1</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R2 starts A2 on C1328 at 2010-02-22 13:01</td>\n",
       "      <td>49.0</td>\n",
       "      <td>A2</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R2 starts A1 on C3185 at 2010-02-23 08:23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A1</td>\n",
       "      <td>R2</td>\n",
       "      <td>C3185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R2 starts A2 on C3185 at 2010-02-23 08:24</td>\n",
       "      <td>50.0</td>\n",
       "      <td>A2</td>\n",
       "      <td>R2</td>\n",
       "      <td>C3185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text   wip activity_nlp resource_nlp  \\\n",
       "0  R2 starts A3 on C1030 at 2010-02-22 12:05  48.0           A3           R2   \n",
       "1  R2 starts A1 on C1328 at 2010-02-22 13:00   0.0           A1           R2   \n",
       "2  R2 starts A2 on C1328 at 2010-02-22 13:01  49.0           A2           R2   \n",
       "3  R2 starts A1 on C3185 at 2010-02-23 08:23   0.0           A1           R2   \n",
       "4  R2 starts A2 on C3185 at 2010-02-23 08:24  50.0           A2           R2   \n",
       "\n",
       "  case_nlp  \n",
       "0    C1030  \n",
       "1    C1328  \n",
       "2    C1328  \n",
       "3    C3185  \n",
       "4    C3185  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "columns = ['text','wip','activity_nlp','resource_nlp','case_nlp']\n",
    "def make_dataset(dataset, iloc_from, iloc_to):\n",
    "    df_ = dataset[columns].iloc[iloc_from:iloc_to]  \n",
    "    df_.rename(columns={\"wip\": \"labels\"})\n",
    "    df_.reset_index(inplace=True,drop=True)\n",
    "    return df_\n",
    "\n",
    "df = pd.read_csv('df_nlp_real.csv',usecols=columns,dtype={'wip':'float'})\n",
    "df_nlp = make_dataset(df, 200,300)\n",
    "display(df_nlp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3aafbfba-f22e-4e1a-802c-125b0caabcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len of tokens:16\n",
      "text  :R2 starts A3 on C1030 at 2010-02-22 12:05 \n",
      "tokens:[101, 30529, 4627, 30522, 2006, 30536, 2012, 2230, 1011, 6185, 1011, 2570, 2260, 1024, 5709, 102] \n",
      "actual token:['[CLS]', 'R2', 'starts', 'A3', 'on', 'C1030', 'at', '2010', '-', '02', '-', '22', '12', ':', '05', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "token_columns = ['activity_nlp','resource_nlp','case_nlp']\n",
    "for x in token_columns:\n",
    "    for y in df_nlp[x].unique():\n",
    "        tokenizer.add_tokens(y)\n",
    "        \n",
    "# df_nlp[\"token\"] = df_nlp[\"text\"].apply(lambda x: tokenizer(x , padding=\"max_length\", truncation=True))\n",
    "df_nlp[\"token\"] = df_nlp[\"text\"].apply(lambda x: tokenizer(x , padding=True, truncation=True))\n",
    "\n",
    "ls = [df_nlp[\"token\"][x]['input_ids'] for x in range(len(df_nlp[\"token\"]))]\n",
    "print(f'max len of tokens:{max([len(x) for x in ls])}')\n",
    "\n",
    "text = df_nlp[\"text\"][0]\n",
    "tokens = tokenizer(text)['input_ids']\n",
    "actual_tokens = [tokenizer.decode(i) for i in tokens]\n",
    "\n",
    "print(f'text  :{text} \\ntokens:{tokens} \\nactual token:{actual_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e7d65-3beb-4c47-98be-c012ab47bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125a0a2-cd1c-4be2-9617-2f3a86591ab6",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8adb61ce-7ed5-44cd-acae-65e1e4057a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1203d29-422d-45b7-940e-e83eebc18ab8",
   "metadata": {},
   "source": [
    "## Pandas To Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b15b756-e9c4-4847-831d-d97c559461f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>activity_nlp</th>\n",
       "      <th>resource_nlp</th>\n",
       "      <th>case_nlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2 starts A3 on C1030 at 2010-02-22 12:05</td>\n",
       "      <td>48.0</td>\n",
       "      <td>A3</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R2 starts A1 on C1328 at 2010-02-22 13:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A1</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R2 starts A2 on C1328 at 2010-02-22 13:01</td>\n",
       "      <td>49.0</td>\n",
       "      <td>A2</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text  labels activity_nlp  \\\n",
       "0  R2 starts A3 on C1030 at 2010-02-22 12:05    48.0           A3   \n",
       "1  R2 starts A1 on C1328 at 2010-02-22 13:00     0.0           A1   \n",
       "2  R2 starts A2 on C1328 at 2010-02-22 13:01    49.0           A2   \n",
       "\n",
       "  resource_nlp case_nlp  \n",
       "0           R2    C1030  \n",
       "1           R2    C1328  \n",
       "2           R2    C1328  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79172563aa82446e88dee7fa0c696525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8c81595e6e49889d6686deacc43e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 70\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "columns = ['text','wip','activity_nlp','resource_nlp','case_nlp']\n",
    "\n",
    "def make_dataset(dataset, iloc_from, iloc_to):\n",
    "    df_ = dataset[columns].iloc[iloc_from:iloc_to]  \n",
    "    df_= df_.rename(columns={\"wip\": \"labels\"})\n",
    "    df_.reset_index(inplace=True,drop=True)\n",
    "    return df_\n",
    "\n",
    "df = pd.read_csv('df_nlp_real.csv',usecols=columns,dtype={'wip':'float'})\n",
    "df_nlp = make_dataset(df, 200,300)\n",
    "display(df_nlp.head(3))\n",
    "\n",
    "dataset = Dataset.from_pandas(df_nlp[['text','labels']],preserve_index=False) \n",
    "dataset = dataset.train_test_split(test_size=0.3) \n",
    "\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ce9bf-e8ec-4fe1-b910-99b3588a24fd",
   "metadata": {},
   "source": [
    "## Tokenization & How To Add New Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f11a98f3-9c09-45c1-9174-14b0a8f44ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924094be0b904162a43df88c1366caae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447eca98aec24735b3790a39d950fc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "token_columns = ['activity_nlp','resource_nlp','case_nlp']\n",
    "for x in token_columns:\n",
    "    for y in df_nlp[x].unique():\n",
    "        tokenizer.add_tokens(y)\n",
    "        \n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4dedd-18a2-415f-9e9b-999d72d7a028",
   "metadata": {},
   "source": [
    "#### Test Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d41ca962-5ff1-4d7f-9567-956b3e815916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text  :R2 starts A3 on C1030 at 2010-02-22 12:05 \n",
      "tokens:[101, 30529, 4627, 30522, 2006, 30536, 2012, 2230, 1011, 6185, 1011, 2570, 2260, 1024, 5709, 102] \n",
      "actual token:['[CLS]', 'R2', 'starts', 'A3', 'on', 'C1030', 'at', '2010', '-', '02', '-', '22', '12', ':', '05', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = df_nlp[\"text\"][0]\n",
    "tokens = tokenizer(text)['input_ids']\n",
    "actual_tokens = [tokenizer.decode(i) for i in tokens]\n",
    "\n",
    "print(f'text  :{text} \\ntokens:{tokens} \\nactual token:{actual_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089c23f-0a6b-4998-8d82-1faf9c02c685",
   "metadata": {},
   "source": [
    "## Fine-Tuning The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff2afcab-4f70-4fc2-9932-ba50d1b2a1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30581, 768)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=1)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b93903-34c9-4d06-8223-8fd379c638fd",
   "metadata": {},
   "source": [
    "### Metrics Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35822364-a1ce-4076-8b89-25e1f87565e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a36c3-026e-49e1-b86c-d66a8838e36e",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "348c5fc7-8152-4fb0-b0c5-90b50263ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/yousef/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 70\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15\n",
      "  Number of trainable parameters = 66999553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 01:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1840.289300</td>\n",
       "      <td>2063.958252</td>\n",
       "      <td>45.430809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1857.195500</td>\n",
       "      <td>2029.629028</td>\n",
       "      <td>45.051407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1835.005900</td>\n",
       "      <td>2017.985474</td>\n",
       "      <td>44.921997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 16\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=1844.1635416666666, metrics={'train_runtime': 106.437, 'train_samples_per_second': 1.973, 'train_steps_per_second': 0.141, 'total_flos': 27817657620480.0, 'train_loss': 1844.1635416666666, 'epoch': 3.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os \n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  logging_strategy=\"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  per_device_eval_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  save_total_limit = 2,\n",
    "                                  save_strategy = 'no',\n",
    "                                  load_best_model_at_end=False\n",
    "                                  )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf6589-5985-4b29-94e0-3821c11f1979",
   "metadata": {},
   "source": [
    "## Save And Load The Pre-Trained Model And Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da49608-1b8f-4b06-a846-3d0233a29e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model/tokenizer\n",
    "\n",
    "model.save_pretrained(\"model\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "\n",
    "# load the model/tokenizer\n",
    "\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4aa8f9-b776-4557-b240-e4a5e03bf686",
   "metadata": {},
   "source": [
    "##  Use The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e81e63d-8d48-4203-b90b-ba10a1fb1bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be43293323874216ae01f0fd566a8000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7360775"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True) \n",
    "\n",
    "def pipeline_prediction(text):\n",
    "    df=pd.DataFrame({'text':[text]})\n",
    "    dataset = Dataset.from_pandas(df,preserve_index=False) \n",
    "    tokenized_datasets = dataset.map(tokenize_function)\n",
    "    raw_pred, _, _ = trainer.predict(tokenized_datasets) \n",
    "    return(raw_pred[0][0])\n",
    "\n",
    "pipeline_prediction(\"üö® Get 50% now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0fe46-e0ea-4e80-a60f-70128b73c66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
