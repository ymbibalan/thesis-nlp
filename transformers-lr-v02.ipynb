{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc700c2-d98b-4bc8-be18-1327dac69c2a",
   "metadata": {},
   "source": [
    "**Reference**:\n",
    "\n",
    "https://towardsdatascience.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface-94b2ed6f798f\n",
    "\n",
    "https://predictivehacks.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125a0a2-cd1c-4be2-9617-2f3a86591ab6",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8adb61ce-7ed5-44cd-acae-65e1e4057a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import transformers\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106628ba-bc97-4065-94d4-4e7ce76cbf39",
   "metadata": {},
   "source": [
    "## Create AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a2bf69bf-1cc4-4721-b6fb-2773701e49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1203d29-422d-45b7-940e-e83eebc18ab8",
   "metadata": {},
   "source": [
    "## Pandas To Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6b15b756-e9c4-4847-831d-d97c559461f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>activity_nlp</th>\n",
       "      <th>resource_nlp</th>\n",
       "      <th>case_nlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2 starts A3 on C1030 at 2010-02-22 12:05</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>A3</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R2 starts A1 on C1328 at 2010-02-22 13:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>A1</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R2 starts A2 on C1328 at 2010-02-22 13:01</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>A2</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text    labels activity_nlp  \\\n",
       "0  R2 starts A3 on C1030 at 2010-02-22 12:05  0.800000           A3   \n",
       "1  R2 starts A1 on C1328 at 2010-02-22 13:00  0.000000           A1   \n",
       "2  R2 starts A2 on C1328 at 2010-02-22 13:01  0.816667           A2   \n",
       "\n",
       "  resource_nlp case_nlp  \n",
       "0           R2    C1030  \n",
       "1           R2    C1328  \n",
       "2           R2    C1328  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e2c7d4907343828124962f4b7d4fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe28240d6574965bf52599c4dce1292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 70\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "columns = ['text','wip','activity_nlp','resource_nlp','case_nlp']\n",
    "\n",
    "def make_dataset(dataset, iloc_from, iloc_to):\n",
    "    df_ = dataset[columns].iloc[iloc_from:iloc_to]  \n",
    "    df_[['wip']] = MinMaxScaler().fit_transform(df_[['wip']])\n",
    "    df_= df_.rename(columns={\"wip\": \"labels\"})\n",
    "\n",
    "    df_.reset_index(inplace=True,drop=True)\n",
    "    return df_\n",
    "\n",
    "df = pd.read_csv('df_nlp_real.csv',usecols=columns,dtype={'wip':'float'})\n",
    "df_nlp = make_dataset(df, 200,300)\n",
    "display(df_nlp.head(3))\n",
    "\n",
    "dataset = Dataset.from_pandas(df_nlp[['text','labels']],preserve_index=False) \n",
    "dataset = dataset.train_test_split(test_size=0.3) \n",
    "\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ce9bf-e8ec-4fe1-b910-99b3588a24fd",
   "metadata": {},
   "source": [
    "## Tokenization & How To Add New Tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f11a98f3-9c09-45c1-9174-14b0a8f44ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7298dff379479382f55c636908aa2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37d55b991b9476aa5b5384a36d8f83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "token_columns = ['activity_nlp','resource_nlp','case_nlp']\n",
    "for x in token_columns:\n",
    "    for y in df_nlp[x].unique():\n",
    "        tokenizer.add_tokens(y)\n",
    "        \n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4dedd-18a2-415f-9e9b-999d72d7a028",
   "metadata": {},
   "source": [
    "#### Test Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d41ca962-5ff1-4d7f-9567-956b3e815916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text  :R2 starts A3 on C1030 at 2010-02-22 12:05 \n",
      "tokens:[101, 30529, 4627, 30522, 2006, 30536, 2012, 2230, 1011, 6185, 1011, 2570, 2260, 1024, 5709, 102] \n",
      "actual token:['[CLS]', 'R2', 'starts', 'A3', 'on', 'C1030', 'at', '2010', '-', '02', '-', '22', '12', ':', '05', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = df_nlp[\"text\"][0]\n",
    "tokens = tokenizer(text)['input_ids']\n",
    "actual_tokens = [tokenizer.decode(i) for i in tokens]\n",
    "\n",
    "print(f'text  :{text} \\ntokens:{tokens} \\nactual token:{actual_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089c23f-0a6b-4998-8d82-1faf9c02c685",
   "metadata": {},
   "source": [
    "## Fine-Tuning The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ff2afcab-4f70-4fc2-9932-ba50d1b2a1d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30581, 768)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# num_labels =1 means regression\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=1)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b93903-34c9-4d06-8223-8fd379c638fd",
   "metadata": {},
   "source": [
    "### Metrics Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "35822364-a1ce-4076-8b89-25e1f87565e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "def compute_metrics_mape(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    mape = mape_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"mape\": mape}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a36c3-026e-49e1-b86c-d66a8838e36e",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "348c5fc7-8152-4fb0-b0c5-90b50263ea1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/yousef/miniforge3/envs/mlp/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 70\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15\n",
      "  Number of trainable parameters = 66999553\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2383, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 0.2794472575187683, 'eval_rmse': 0.5286276936531067, 'eval_runtime': 3.4499, 'eval_samples_per_second': 8.696, 'eval_steps_per_second': 0.58, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.159, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 0.17678657174110413, 'eval_rmse': 0.42045995593070984, 'eval_runtime': 3.4631, 'eval_samples_per_second': 8.663, 'eval_steps_per_second': 0.578, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2052, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1526743769645691, 'eval_rmse': 0.3907356560230255, 'eval_runtime': 3.4524, 'eval_samples_per_second': 8.69, 'eval_steps_per_second': 0.579, 'epoch': 3.0}\n",
      "{'train_runtime': 108.1363, 'train_samples_per_second': 1.942, 'train_steps_per_second': 0.139, 'train_loss': 0.2008363167444865, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=0.2008363167444865, metrics={'train_runtime': 108.1363, 'train_samples_per_second': 1.942, 'train_steps_per_second': 0.139, 'train_loss': 0.2008363167444865, 'epoch': 3.0})"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  logging_strategy=\"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  per_device_eval_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  save_total_limit = 2,\n",
    "                                  save_strategy = 'no',\n",
    "                                  load_best_model_at_end=False,\n",
    "                                  report_to=\"none\"\n",
    "                                  )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b12ee3-b66c-46c1-884d-cc5a01133059",
   "metadata": {},
   "source": [
    "# Train and test chart and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fed1d-5a7f-441d-9066-8693246110d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 30\n",
      "  Batch size = 16\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 70\n",
      "  Batch size = 16\n"
     ]
    }
   ],
   "source": [
    "predictions_test = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions_train = trainer.predict(tokenized_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5f190-ef8e-446e-9646-b97bef3f4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [predictions_train.predictions,predictions_test.predictions]\n",
    "fig, axs = plt.subplots(nrows=1,ncols=2,figsize=(10, 5))\n",
    "\n",
    "axs[0].plot(data[0],label='train')\n",
    "axs[0].title.set_text('train ')\n",
    "\n",
    "axs[1].plot(data[1],label='train')\n",
    "axs[1].title.set_text('test ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70e588-5e9c-4118-9a1b-3726154bd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'key\\t\\t train.metric \\t test.metrics')\n",
    "\n",
    "for key in list(predictions_test.metrics.keys())[:2]:\n",
    "    print(f'{key} \\t\\t{predictions_train.metrics[key]:0.3f} \\t\\t{predictions_test.metrics[key]:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf6589-5985-4b29-94e0-3821c11f1979",
   "metadata": {},
   "source": [
    "## Save And Load The Pre-Trained Model And Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da49608-1b8f-4b06-a846-3d0233a29e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model/tokenizer\n",
    "\n",
    "model.save_pretrained(\"model\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "\n",
    "# load the model/tokenizer\n",
    "\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4aa8f9-b776-4557-b240-e4a5e03bf686",
   "metadata": {},
   "source": [
    "##  Use The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81e63d-8d48-4203-b90b-ba10a1fb1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True) \n",
    "\n",
    "def pipeline_prediction(text):\n",
    "    df=pd.DataFrame({'text':[text]})\n",
    "    dataset = Dataset.from_pandas(df,preserve_index=False) \n",
    "    tokenized_datasets = dataset.map(tokenize_function)\n",
    "    raw_pred, _, _ = trainer.predict(tokenized_datasets) \n",
    "    return(raw_pred[0][0])\n",
    "\n",
    "pipeline_prediction(\"ðŸš¨ Get 50% now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0fe46-e0ea-4e80-a60f-70128b73c66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
