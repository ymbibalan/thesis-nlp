{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc700c2-d98b-4bc8-be18-1327dac69c2a",
   "metadata": {},
   "source": [
    "**Reference**:\n",
    "\n",
    "https://towardsdatascience.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface-94b2ed6f798f\n",
    "\n",
    "https://predictivehacks.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a327bd2-6014-4f80-802d-cc90a3c1b736",
   "metadata": {},
   "source": [
    "# Don't normalize the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125a0a2-cd1c-4be2-9617-2f3a86591ab6",
   "metadata": {},
   "source": [
    "# Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8adb61ce-7ed5-44cd-acae-65e1e4057a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f698ca-fca4-45b8-a900-89e0bb884987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yousef/code/thesis-nlp/code-wip-dataset\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f600af6-4e0c-4ce1-8ed8-dffc1dc48493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number in the data is 366\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_open</th>\n",
       "      <th>w_high</th>\n",
       "      <th>w_low</th>\n",
       "      <th>w_close</th>\n",
       "      <th>new</th>\n",
       "      <th>started</th>\n",
       "      <th>done</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>target</th>\n",
       "      <th>y_ystrdy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            w_open  w_high  w_low  w_close  new  started  done  dayofweek  \\\n",
       "date                                                                        \n",
       "2010-01-14     0.0     1.0    0.0      1.0  3.0      1.0   0.0          3   \n",
       "2010-01-15     1.0     1.0    1.0      1.0  1.0      0.0   0.0          4   \n",
       "\n",
       "            dayofmonth  dayofyear  target  y_ystrdy  \n",
       "date                                                 \n",
       "2010-01-14          14         14     1.0       0.0  \n",
       "2010-01-15          15         15     1.0       2.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>w_open 0 w_high 1 w_low 0 w_close 1 new 3 started 1 done 0 dayofweek 3 dayofmonth 14 dayofyear 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>w_open 1 w_high 1 w_low 1 w_close 1 new 1 started 0 done 0 dayofweek 4 dayofmonth 15 dayofyear 15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label  \\\n",
       "date                \n",
       "2010-01-14    1.0   \n",
       "2010-01-15    1.0   \n",
       "\n",
       "                                                                                                          text  \n",
       "date                                                                                                            \n",
       "2010-01-14   w_open 0 w_high 1 w_low 0 w_close 1 new 3 started 1 done 0 dayofweek 3 dayofmonth 14 dayofyear 14  \n",
       "2010-01-15   w_open 1 w_high 1 w_low 1 w_close 1 new 1 started 0 done 0 dayofweek 4 dayofmonth 15 dayofyear 15  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FILE_PATH = '/Users/yousef/code/thesis-nlp/dataset/incident-ohlc-v06-scalar.csv'\n",
    "COLUMNS =  ['w_open', 'w_high', 'w_low', 'w_close', 'new', 'started','done', 'dayofweek', 'dayofmonth', 'dayofyear', 'target', 'y_ystrdy']\n",
    "\n",
    "# N_ROWS = 100\n",
    "# df_ = pd.read_csv(FILE_PATH,parse_dates=['date'],index_col=['date'], nrows=N_ROWS)\n",
    "df_ = pd.read_csv(FILE_PATH,parse_dates=['date'],index_col=['date'])\n",
    "df_ = (df_.sort_index())[COLUMNS]\n",
    "df_ = df_.iloc[1:-1]  # Remove NaN\n",
    "\n",
    "max_number_in_data = int(df_.describe().transpose()[\"max\"].max())\n",
    "print(f'Max number in the data is {max_number_in_data}')\n",
    "display(df_.head(2))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['label']= df_['target']\n",
    "df['text']= ''\n",
    "for col in COLUMNS[:-2]:\n",
    "    df_[col] = df_[col].apply(lambda x: f'{x:.0f}')\n",
    "    df['text'] = df['text'] + f' {col} ' + df_[col].astype(str)\n",
    "\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb3c0ab-a72b-4911-a604-f459aa16c026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                                                                                                   6.0\n",
       "text      w_open 3 w_high 4 w_low 3 w_close 4 new 0 started 1 done 0 dayofweek 2 dayofmonth 20 dayofyear 20\n",
       "Name: 2010-01-20 00:00:00, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3ccdd7-eece-4416-b02b-1cfaa92c5e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ','.join(df[['text']])\n",
    "# df['text']\n",
    "# ' '.join(df['text'][0:3].to_list())\n",
    "# # df['label'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106628ba-bc97-4065-94d4-4e7ce76cbf39",
   "metadata": {},
   "source": [
    "## Create and customize AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2bf69bf-1cc4-4721-b6fb-2773701e49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "for x in COLUMNS:\n",
    "    tokenizer.add_tokens(x)\n",
    "    \n",
    "for x in range(max_number_in_data):\n",
    "    tokenizer.add_tokens(str(x))\n",
    "    \n",
    "        \n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1203d29-422d-45b7-940e-e83eebc18ab8",
   "metadata": {},
   "source": [
    "## Pandas To Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b15b756-e9c4-4847-831d-d97c559461f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 135 TEST: 131\n",
      "TRAIN: 266 TEST: 131\n",
      "TRAIN: 397 TEST: 131\n",
      "TRAIN: 528 TEST: 131\n",
      "TRAIN: 659 TEST: 131\n",
      "TRAIN: 790 TEST: 131\n",
      "TRAIN: 921 TEST: 131\n",
      "TRAIN: 1052 TEST: 131\n",
      "TRAIN: 1183 TEST: 131\n",
      "TRAIN: 1314 TEST: 131\n",
      "length of datasets is 10\n",
      "{Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 135\n",
      "})}\n",
      "{Dataset({\n",
      "    features: ['label', 'text'],\n",
      "    num_rows: 131\n",
      "})}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "df_lb = pd.DataFrame()\n",
    "df_lb['label'] = ''\n",
    "df_lb['text'] = ''\n",
    "\n",
    "LOOKBACK_WINDOW =5\n",
    "for i in range(LOOKBACK_WINDOW, len(df)):\n",
    "    text = ' '.join(df['text'][i - LOOKBACK_WINDOW:i].to_list())\n",
    "    label = df['label'][i]\n",
    "    df_lb.loc[len(df_lb.index)] = [label,text] \n",
    "\n",
    "datasets = []\n",
    "N_SPLIT = 10\n",
    "tscv = TimeSeriesSplit(n_splits=N_SPLIT)\n",
    "for train_index, test_index in tscv.split(df_lb):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    df_tr = df_lb.iloc[train_index]\n",
    "    df_ts = df_lb.iloc[test_index]\n",
    "    tr_ds = Dataset.from_pandas(df_tr, split=\"train\",preserve_index=False)\n",
    "    ts_ds = Dataset.from_pandas(df_ts, split=\"test\",preserve_index=False)\n",
    "    datasets.append({'train':tr_ds,'test':ts_ds})\n",
    "\n",
    "print(f'length of datasets is {len(datasets)}')\n",
    "print({datasets[0]['train']})\n",
    "print({datasets[0]['test']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089c23f-0a6b-4998-8d82-1faf9c02c685",
   "metadata": {},
   "source": [
    "## Fine-Tuning The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b93903-34c9-4d06-8223-8fd379c638fd",
   "metadata": {},
   "source": [
    "### Metrics Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35822364-a1ce-4076-8b89-25e1f87565e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    rmse = mean_squared_error(labels, predictions, squared=True)\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "def compute_metrics_mape(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    mape = mape_metric.compute(predictions=predictions, references=labels)\n",
    "    return {\"mape\": mape}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff2afcab-4f70-4fc2-9932-ba50d1b2a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_labels =1 means regression\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=1)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  logging_strategy=\"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  per_device_eval_batch_size=16,\n",
    "                                  num_train_epochs=3,\n",
    "                                  save_total_limit = 2,\n",
    "                                  save_strategy = 'no',\n",
    "                                  load_best_model_at_end=False,\n",
    "                                  report_to=\"none\",\n",
    "                                  optim=\"adamw_torch\",\n",
    "                                  # optim=\"adagrad\"\n",
    "                                  \n",
    "\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a36c3-026e-49e1-b86c-d66a8838e36e",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c5fc7-8152-4fb0-b0c5-90b50263ea1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fbd8f648a34c4ebff71632db1b8467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0930cf59a0a4127852b07678f24d6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 135\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 27\n",
      "  Number of trainable parameters = 66969601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<= Training 0 of 10 .... =>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5714.3286, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 9944.43359375, 'eval_rmse': 9944.43359375, 'eval_runtime': 16.7699, 'eval_samples_per_second': 7.812, 'eval_steps_per_second': 0.537, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5340.3021, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 9724.4609375, 'eval_rmse': 9724.4609375, 'eval_runtime': 17.3918, 'eval_samples_per_second': 7.532, 'eval_steps_per_second': 0.517, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5325.852, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 135\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9656.8359375, 'eval_rmse': 9656.8349609375, 'eval_runtime': 16.2174, 'eval_samples_per_second': 8.078, 'eval_steps_per_second': 0.555, 'epoch': 3.0}\n",
      "{'train_runtime': 238.984, 'train_samples_per_second': 1.695, 'train_steps_per_second': 0.113, 'train_loss': 5460.16087962963, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5312.42333984375, 'eval_rmse': 5312.421875, 'eval_runtime': 16.7429, 'eval_samples_per_second': 8.063, 'eval_steps_per_second': 0.538, 'epoch': 3.0}\n",
      "{'eval_loss': 9656.8359375, 'eval_rmse': 9656.8349609375, 'eval_runtime': 16.3758, 'eval_samples_per_second': 8.0, 'eval_steps_per_second': 0.55, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2580698064cd4f0dafe1527df822b814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabc7de0e6d54f65b5f6dce0aeadaaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 266\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 51\n",
      "  Number of trainable parameters = 66969601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<= Training 1 of 10 .... =>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7297.0528, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 19515.29296875, 'eval_rmse': 19515.294921875, 'eval_runtime': 16.0087, 'eval_samples_per_second': 8.183, 'eval_steps_per_second': 0.562, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7016.4426, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 19191.33984375, 'eval_rmse': 19191.33984375, 'eval_runtime': 16.7533, 'eval_samples_per_second': 7.819, 'eval_steps_per_second': 0.537, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6929.0028, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 266\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 19076.654296875, 'eval_rmse': 19076.65625, 'eval_runtime': 16.5329, 'eval_samples_per_second': 7.924, 'eval_steps_per_second': 0.544, 'epoch': 3.0}\n",
      "{'train_runtime': 414.4615, 'train_samples_per_second': 1.925, 'train_steps_per_second': 0.123, 'train_loss': 7080.832720588235, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6874.45458984375, 'eval_rmse': 6874.4541015625, 'eval_runtime': 33.9205, 'eval_samples_per_second': 7.842, 'eval_steps_per_second': 0.501, 'epoch': 3.0}\n",
      "{'eval_loss': 19076.654296875, 'eval_rmse': 19076.65625, 'eval_runtime': 16.8757, 'eval_samples_per_second': 7.763, 'eval_steps_per_second': 0.533, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5fcbd1eafe441fa36e2a4712ca185f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2512729f4e454add90fbee42c3446258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 397\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 75\n",
      "  Number of trainable parameters = 66969601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<= Training 2 of 10 .... =>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10643.0, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 28529.65234375, 'eval_rmse': 28529.65234375, 'eval_runtime': 16.5806, 'eval_samples_per_second': 7.901, 'eval_steps_per_second': 0.543, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10118.6756, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 27841.40234375, 'eval_rmse': 27841.400390625, 'eval_runtime': 16.4756, 'eval_samples_per_second': 7.951, 'eval_steps_per_second': 0.546, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9850.415, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 397\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 27601.236328125, 'eval_rmse': 27601.236328125, 'eval_runtime': 16.5137, 'eval_samples_per_second': 7.933, 'eval_steps_per_second': 0.545, 'epoch': 3.0}\n",
      "{'train_runtime': 600.7858, 'train_samples_per_second': 1.982, 'train_steps_per_second': 0.125, 'train_loss': 10204.030208333334, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9790.591796875, 'eval_rmse': 9790.591796875, 'eval_runtime': 51.3265, 'eval_samples_per_second': 7.735, 'eval_steps_per_second': 0.487, 'epoch': 3.0}\n",
      "{'eval_loss': 27601.236328125, 'eval_rmse': 27601.236328125, 'eval_runtime': 17.0721, 'eval_samples_per_second': 7.673, 'eval_steps_per_second': 0.527, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fac8485e5a946c681b9286897939ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f856a47d2a46f4a9643153f8b8703b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 528\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 99\n",
      "  Number of trainable parameters = 66969601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<= Training 3 of 10 .... =>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 13670.8883, 'learning_rate': 3.3333333333333335e-05, 'epoch': 1.0}\n",
      "{'eval_loss': 18833.166015625, 'eval_rmse': 18833.16796875, 'eval_runtime': 16.9673, 'eval_samples_per_second': 7.721, 'eval_steps_per_second': 0.53, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12744.1136, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'eval_loss': 17958.390625, 'eval_rmse': 17958.392578125, 'eval_runtime': 17.1228, 'eval_samples_per_second': 7.651, 'eval_steps_per_second': 0.526, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 12271.8892, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 528\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 17657.205078125, 'eval_rmse': 17657.203125, 'eval_runtime': 16.5998, 'eval_samples_per_second': 7.892, 'eval_steps_per_second': 0.542, 'epoch': 3.0}\n",
      "{'train_runtime': 776.1483, 'train_samples_per_second': 2.041, 'train_steps_per_second': 0.128, 'train_loss': 12895.630366161617, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 131\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 12176.06640625, 'eval_rmse': 12176.06640625, 'eval_runtime': 68.0673, 'eval_samples_per_second': 7.757, 'eval_steps_per_second': 0.485, 'epoch': 3.0}\n",
      "{'eval_loss': 17657.205078125, 'eval_rmse': 17657.203125, 'eval_runtime': 16.8956, 'eval_samples_per_second': 7.754, 'eval_steps_per_second': 0.533, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2753e855e34f4586aaef82a4b25c8285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a09612827d4385b76f2c8102a089b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 659\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 126\n",
      "  Number of trainable parameters = 66969601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<= Training 4 of 10 .... =>\n"
     ]
    }
   ],
   "source": [
    "train_output = []\n",
    "test_output = []\n",
    "\n",
    "transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "for idx, ds in enumerate(datasets):\n",
    "    tokenized_train_ds = ds['train'].map(tokenize_function, batched=True)\n",
    "    tokenized_test_ds = ds['test'].map(tokenize_function, batched=True)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_ds,\n",
    "        eval_dataset=tokenized_test_ds,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    print(f'\\n<= Training {idx} of {len(datasets)} .... =>')\n",
    "    trainer.train()\n",
    "    train_output.append(trainer.evaluate(tokenized_train_ds))\n",
    "    test_output.append(trainer.evaluate())\n",
    "print('\\n<= Train is done =>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec9a45-56dd-4155-ba91-52450b87a30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_loss = [x.training_loss for x in train_output]\n",
    "tr_loss = [x['eval_loss'] for x in train_output]\n",
    "ts_loss = [x['eval_loss'] for x in test_output]\n",
    "plt.plot(tr_loss,label='tr_loss')\n",
    "plt.plot(ts_loss,label='ts_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7915aeb-8d69-4d4e-beb3-913cf762d68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_loss = [x.metrics['train_runtime'] for x in train_output]\n",
    "ts_loss = [x['eval_rmse'] for x in train_output]\n",
    "ts_loss = [x['eval_rmse'] for x in test_output]\n",
    "plt.plot(tr_loss,label='tr_rmse')\n",
    "plt.plot(ts_loss,label='ts_rmse')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d411393f-e115-4d2d-876a-8bf5736fa7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba15ec4-2de9-4c07-968b-b558ab8ee175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_test_ds['text']\n",
    "# tokenized_test_ds['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b12ee3-b66c-46c1-884d-cc5a01133059",
   "metadata": {},
   "source": [
    "# Train and test chart and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774fed1d-5a7f-441d-9066-8693246110d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n<= Predicting =>')\n",
    "predictions_test = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions_train = trainer.predict(tokenized_datasets[\"train\"])\n",
    "print('\\n<= Predictions are done =>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5f190-ef8e-446e-9646-b97bef3f4ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [predictions_train.predictions,predictions_test.predictions]\n",
    "fig, axs = plt.subplots(nrows=1,ncols=2,figsize=(10, 5))\n",
    "\n",
    "axs[0].plot(data[0],label='train')\n",
    "axs[0].plot(dataset[\"train\"]['label'],label='y')\n",
    "axs[0].title.set_text('train ')\n",
    "axs[0].legend(loc='upper right')\n",
    "\n",
    "\n",
    "axs[1].plot(data[1],label='test')\n",
    "axs[1].plot(dataset[\"test\"]['label'],label='y')\n",
    "axs[1].title.set_text('test ')\n",
    "axs[1].legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0aaac0-9e77-4671-a15d-1d12c5c2b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictions_test.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70e588-5e9c-4118-9a1b-3726154bd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'key\\t\\t train.metric \\t test.metrics')\n",
    "\n",
    "for key in list(predictions_test.metrics.keys())[:2]:\n",
    "    print(f'{key} \\t\\t{predictions_train.metrics[key]:0.3f} \\t\\t{predictions_test.metrics[key]:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf6589-5985-4b29-94e0-3821c11f1979",
   "metadata": {},
   "source": [
    "## Save And Load The Pre-Trained Model And Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da49608-1b8f-4b06-a846-3d0233a29e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model/tokenizer\n",
    "\n",
    "model.save_pretrained(\"model\")\n",
    "tokenizer.save_pretrained(\"tokenizer\")\n",
    "\n",
    "# load the model/tokenizer\n",
    "\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4aa8f9-b776-4557-b240-e4a5e03bf686",
   "metadata": {},
   "source": [
    "##  Use The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81e63d-8d48-4203-b90b-ba10a1fb1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True) \n",
    "\n",
    "def pipeline_prediction(text):\n",
    "    df=pd.DataFrame({'text':[text]})\n",
    "    dataset = Dataset.from_pandas(df,preserve_index=False) \n",
    "    tokenized_datasets = dataset.map(tokenize_function)\n",
    "    raw_pred, _, _ = trainer.predict(tokenized_datasets) \n",
    "    return(raw_pred[0][0])\n",
    "\n",
    "pipeline_prediction(\"ðŸš¨ Get 50% now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0fe46-e0ea-4e80-a60f-70128b73c66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
