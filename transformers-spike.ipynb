{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc700c2-d98b-4bc8-be18-1327dac69c2a",
   "metadata": {},
   "source": [
    "**Reference**:\n",
    "\n",
    "https://towardsdatascience.com/how-to-fine-tune-an-nlp-regression-model-with-transformers-and-huggingface-94b2ed6f798f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9040a6e2-3a79-4bd9-b7df-47819a718bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabfdd39-ebb4-40d9-a637-37e841b213ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ü§ó Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c3352e-ca99-42ac-81f8-8dce6bb19571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], [101, 11312, 18763, 10855, 11530, 112, 162, 39487, 10197, 119, 102, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    [\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419f325d-0575-42d4-b171-27d108baefbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 18:31:38.618923: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-12-07 18:31:38.619273: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFDistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-cased-distilled-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9153627157211304, 'start': 66, 'end': 90, 'answer': 'Brookline, Massachusetts'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Allocate a pipeline for question-answering\n",
    "question_answerer = pipeline('question-answering')\n",
    "\n",
    "# Ask a question\n",
    "answer = question_answerer({\n",
    "\t'question': 'Where is KDnuggets headquartered?',\n",
    "\t'context': 'KDnuggets was founded in February of 1997 by Gregory Piatetsky in Brookline, Massachusetts.'\n",
    "})\n",
    "\n",
    "# Print the answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575ca416-3b33-4310-bb88-a8bff86ae91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset,load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff84807a-39bb-4ba8-8aff-96e4e336e407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens      :[101, 100, 2238, 4530, 2444, 100, 102]\n",
      "actual token:['[CLS]', '[UNK]', 'june', 'drop', 'live', '[UNK]', '[SEP]']\n",
      "revised actual tokens:['[CLS]', 'üö®', 'june', 'drop', 'live', 'üö®', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokens = tokenizer('üö® JUNE DROP LIVE üö®')['input_ids']\n",
    "actual_tokens = [tokenizer.decode(i) for i in tokenizer('üö® JUNE DROP LIVE üö®')['input_ids']]\n",
    "\n",
    "print(f'tokens      :{tokens}')\n",
    "print(f'actual token:{actual_tokens}')\n",
    "\n",
    "for i in ['üö®', 'üôÇ', 'üòç', '‚úåÔ∏è' , 'ü§© ']:\n",
    "    tokenizer.add_tokens(i)\n",
    "revised_actual_tokens = [tokenizer.decode(i) for i in tokenizer('üö® JUNE DROP LIVE üö®')['input_ids']]\n",
    "\n",
    "# Now, if you tokenize the sentence you will see that the emoji remains as emoji and not the [UNK] token.\n",
    "print(f'revised actual tokens:{revised_actual_tokens}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a68423f8-a8a4-41f3-ad95-06bbcd3b85d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ = pd.read_csv('df_nlp_real.csv')\n",
    "# df_.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c7893e1-2566-4294-a134-827ce8f78ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>wip</th>\n",
       "      <th>activity_nlp</th>\n",
       "      <th>resource_nlp</th>\n",
       "      <th>case_nlp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R2 starts A3 on C1030 at 2010-02-22 12:05</td>\n",
       "      <td>48.0</td>\n",
       "      <td>A3</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R2 starts A1 on C1328 at 2010-02-22 13:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A1</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R2 starts A2 on C1328 at 2010-02-22 13:01</td>\n",
       "      <td>49.0</td>\n",
       "      <td>A2</td>\n",
       "      <td>R2</td>\n",
       "      <td>C1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R2 starts A1 on C3185 at 2010-02-23 08:23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A1</td>\n",
       "      <td>R2</td>\n",
       "      <td>C3185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R2 starts A2 on C3185 at 2010-02-23 08:24</td>\n",
       "      <td>50.0</td>\n",
       "      <td>A2</td>\n",
       "      <td>R2</td>\n",
       "      <td>C3185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text   wip activity_nlp resource_nlp  \\\n",
       "0  R2 starts A3 on C1030 at 2010-02-22 12:05  48.0           A3           R2   \n",
       "1  R2 starts A1 on C1328 at 2010-02-22 13:00   0.0           A1           R2   \n",
       "2  R2 starts A2 on C1328 at 2010-02-22 13:01  49.0           A2           R2   \n",
       "3  R2 starts A1 on C3185 at 2010-02-23 08:23   0.0           A1           R2   \n",
       "4  R2 starts A2 on C3185 at 2010-02-23 08:24  50.0           A2           R2   \n",
       "\n",
       "  case_nlp  \n",
       "0    C1030  \n",
       "1    C1328  \n",
       "2    C1328  \n",
       "3    C3185  \n",
       "4    C3185  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "columns = ['text','wip','activity_nlp','resource_nlp','case_nlp']\n",
    "def make_dataset(dataset, iloc_from, iloc_to):\n",
    "    df_ = dataset[columns].iloc[iloc_from:iloc_to]  \n",
    "    df_.rename(columns={\"wip\": \"labels\"})\n",
    "    df_.reset_index(inplace=True,drop=True)\n",
    "    return df_\n",
    "\n",
    "df = pd.read_csv('df_nlp_real.csv',usecols=columns,dtype={'wip':'float'})\n",
    "df_nlp = make_dataset(df, 200,300)\n",
    "display(df_nlp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aafbfba-f22e-4e1a-802c-125b0caabcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len of tokens:16\n",
      "text  :R2 starts A3 on C1030 at 2010-02-22 12:05 \n",
      "tokens:[101, 30529, 4627, 30522, 2006, 30536, 2012, 2230, 1011, 6185, 1011, 2570, 2260, 1024, 5709, 102] \n",
      "actual token:['[CLS]', 'R2', 'starts', 'A3', 'on', 'C1030', 'at', '2010', '-', '02', '-', '22', '12', ':', '05', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "token_columns = ['activity_nlp','resource_nlp','case_nlp']\n",
    "for x in token_columns:\n",
    "    for y in df_nlp[x].unique():\n",
    "        tokenizer.add_tokens(y)\n",
    "        \n",
    "# df_nlp[\"token\"] = df_nlp[\"text\"].apply(lambda x: tokenizer(x , padding=\"max_length\", truncation=True))\n",
    "df_nlp[\"token\"] = df_nlp[\"text\"].apply(lambda x: tokenizer(x , padding=True, truncation=True))\n",
    "\n",
    "ls = [df_nlp[\"token\"][x]['input_ids'] for x in range(len(df_nlp[\"token\"]))]\n",
    "print(f'max len of tokens:{max([len(x) for x in ls])}')\n",
    "\n",
    "text = df_nlp[\"text\"][0]\n",
    "tokens = tokenizer(text)['input_ids']\n",
    "actual_tokens = [tokenizer.decode(i) for i in tokens]\n",
    "\n",
    "print(f'text  :{text} \\ntokens:{tokens} \\nactual token:{actual_tokens}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7e7d65-3beb-4c47-98be-c012ab47bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca27da9-6ad1-4a07-bebd-a8491e45573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x,row in df_nlp.iterrows():\n",
    "    print(len(row['token']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2afcab-4f70-4fc2-9932-ba50d1b2a1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "mlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
